{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this will be the folder_name\n",
    "model_folder_name=\"/LR\"\n",
    "\n",
    "#use this to run several models if desired\n",
    "model_name=\"LR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import auc,roc_curve,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# list experiment parameters\n",
    "# note that if the number of parties is different than 3 - \n",
    "# you need to create the required configuration files that are used by the viff package\n",
    "#############################\n",
    "parties=3\n",
    "\n",
    "public_path=os.getcwd()+\"/MyDistExperiment/public\"\n",
    "private_path=os.getcwd()+\"/MyDistExperiment/private/\" + str(parties) + \"p\"\n",
    "\n",
    "ds_path=os.getcwd()+\"/MyDistExperiment/datasets\"\n",
    "\n",
    "lams=list(np.around(np.arange(0.0,1.1,0.1),decimals=2))\n",
    "print(lams)\n",
    "\n",
    "#repairs=[\"III_min\",\"I_min\",\"I_med\"]\n",
    "repairs=[\"III_min\"]\n",
    "\n",
    "#datasets=[\"synthetic9\"]\n",
    "#datasets=[\"propublica-recidivism_numerical-binsensitive_race_1attr\"]\n",
    "datasets=[\"propublica-recidivism_numerical-binsensitive_race\"]\n",
    "\n",
    "print(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "# after choosing a dataset\n",
    "# we split it with the same random seed to train and test\n",
    "# we split it with the same random seed to each of the parties\n",
    "# here we define the functions for creating these partitions\n",
    "#############################\n",
    "\n",
    "def create_train_test_splits(num,TRAINING_PERCENT,df):\n",
    "    splits=[]\n",
    "    for i in range(0, num):\n",
    "        # we first shuffle a list of indices so that each subprocessed data\n",
    "        # is split consistently\n",
    "        n=len(df)\n",
    "\n",
    "        #for each i use i as seed for the shuffle\n",
    "        a = np.arange(n)\n",
    "        np.random.seed(i)\n",
    "        np.random.shuffle(a)\n",
    "\n",
    "        split_ix = int(n * TRAINING_PERCENT)\n",
    "        train_fraction = a[:split_ix]\n",
    "        test_fraction = a[split_ix:]\n",
    "\n",
    "        train = df.iloc[train_fraction]\n",
    "        test = df.iloc[test_fraction]\n",
    "        \n",
    "        splits.append((train, test))\n",
    "\n",
    "    return splits\n",
    "\n",
    "###########################\n",
    "#we usually use num=1 since the reuslts are exact in our MPC repair process\n",
    "###########################\n",
    "\n",
    "def create_random_splits_for_parties(df,num=1,PARTIES=parties,SPLIT_PERCENTS=None):\n",
    "    if SPLIT_PERCENTS is None:\n",
    "        SPLIT_PERCENTS=[1.0/PARTIES]*PARTIES\n",
    "        \n",
    "    if sum(SPLIT_PERCENTS)>1:\n",
    "        raise RuntimeError(\"The sum of the inputs of split percents is larger than 1\")\n",
    "    if sum(SPLIT_PERCENTS)<0.9999999999999:\n",
    "        raise RuntimeError(\"The sum of the inputs of split percents is lower than 1\")\n",
    "    if len(SPLIT_PERCENTS)>PARTIES:\n",
    "        raise RuntimeError(\"The length of the inputs of split percents is larger than the number of parties\")\n",
    "    if len(SPLIT_PERCENTS)<PARTIES:\n",
    "        raise RuntimeError(\"The length of the inputs of split percents is lower than the number of parties\")\n",
    "\n",
    "        \n",
    "    splits=[]\n",
    "    for i in range(0, num):\n",
    "        # we first shuffle a list of indices so that each subprocessed data\n",
    "        # is split consistently\n",
    "        n=len(df)\n",
    "\n",
    "        #for each i use i as seed for the shuffle\n",
    "        a = np.arange(n)\n",
    "        np.random.seed(i+100)\n",
    "        np.random.shuffle(a)\n",
    "\n",
    "        parties_dfs = []\n",
    "        start_split_ix=0\n",
    "        end_split_ix=0\n",
    "        for ell in range(0,PARTIES):\n",
    "            end_split_ix=end_split_ix+int(n*SPLIT_PERCENTS[ell])\n",
    "            if (ell==PARTIES-1):\n",
    "                end_split_ix=n\n",
    "            #print(end_split_ix)\n",
    "            party_fraction=a[start_split_ix:end_split_ix]\n",
    "            party_df=df.iloc[party_fraction]\n",
    "            parties_dfs.append(party_df)\n",
    "            start_split_ix=end_split_ix\n",
    "            #print(parties_dfs)\n",
    "    \n",
    "        splits.append(parties_dfs)\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for dataset_name in datasets:\n",
    "    \n",
    "    ####################\n",
    "    # define dataset related properties\n",
    "    # note: ignore_features are the features to ignore in the repair stage\n",
    "    ####################\n",
    "    \n",
    "    df=None\n",
    "    \n",
    "    dataset_path=ds_path+\"/\" + dataset_name + \".csv\"\n",
    "    df=pd.read_table(dataset_path,delimiter=\",\")\n",
    "    print(dataset_name)\n",
    "    \n",
    "    \n",
    "    if dataset_name.startswith((\"synthetic\")):\n",
    "        sensitive_feature=\"Socioeconomic_status\"\n",
    "        priv_value=\"Privileged\"\n",
    "        target_feature=\"Success\"\n",
    "        positive_class_val=0\n",
    "        ignore_features=[]\n",
    "\n",
    "\n",
    "    if dataset_name.startswith((\"propublica-recidivism\")):\n",
    "        sensitive_feature=\"race\"\n",
    "        #sensitive_feature=\"sex\"\n",
    "        priv_value=1\n",
    "        target_feature=\"two_year_recid\"\n",
    "        positive_class_val=1\n",
    "        #positive_class_val=0\n",
    "        ignore_features=[]\n",
    "\n",
    "        \n",
    "\n",
    "    # features - all features that are not in the target, sensitive, ignore\n",
    "    all_features=[col for col in df.columns if col not in [sensitive_feature,target_feature,ignore_features]]\n",
    "    print(all_features)\n",
    "\n",
    "    alphas=[]\n",
    "    betas=[]\n",
    "    max_bin_sizes=[]\n",
    "    features=[]\n",
    "    for attr in all_features:\n",
    "        #number of unique values\n",
    "        num_vals=len(list(set(df[attr])))\n",
    "        if num_vals>2:\n",
    "            features.append(attr)\n",
    "            alphas.append(min(df[attr]))\n",
    "            betas.append(max(df[attr]))\n",
    "            max_bin_sizes.append(num_vals)\n",
    "        else:\n",
    "            ignore_features.append(attr)\n",
    "\n",
    "    print(ignore_features)\n",
    "    print(features)\n",
    "    print(alphas)\n",
    "    print(betas)    \n",
    "\n",
    " \n",
    "    print(\"######\")\n",
    "    print(\"#bins#\")\n",
    "    print(\"######\")\n",
    "\n",
    "    #####\n",
    "    #only at most 10% of the min between sum(priv_sizes) and sum(unpriv_sizes), since B<<nu,nv\n",
    "    #####\n",
    "\n",
    "    n_v=sum(df[sensitive_feature]==priv_value)\n",
    "    n_u=sum(df[sensitive_feature]!=priv_value)\n",
    "    print(n_v,n_u)\n",
    "\n",
    "    lower_n=min(n_u,n_v)\n",
    "    print(lower_n)\n",
    "    bin_nums_list=[]\n",
    "\n",
    "    bnlst=[1,2,3,4,6,8,9,10]\n",
    "\n",
    "    print(bnlst)\n",
    "    \n",
    "    for b in bnlst:\n",
    "        bin_nums=[b]*len(features)\n",
    "        bin_nums_list.append(bin_nums)\n",
    "\n",
    "    print(bin_nums_list)\n",
    "    print(len(bin_nums_list))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #####################################\n",
    "    # creating partitioning for parties\n",
    "    #####################################\n",
    "\n",
    "    df_splits=create_random_splits_for_parties(df,1,parties,[1/parties]*parties)\n",
    "\n",
    "    priv_sizes=[]\n",
    "    unpriv_sizes=[]\n",
    "    for ell in range(0,len(df_splits[0])):\n",
    "        df_splits[0][ell].insert(len(df_splits[0][ell].columns),\"party\",ell)\n",
    "        df_party=df_splits[0][ell]\n",
    "        df_party.to_csv(private_path+\"/party\"+str(ell)+\"/\"+dataset_name+\"_partial.csv\",index=True,index_label=\"index\")\n",
    "        priv_sizes.append(sum(df_party[sensitive_feature]==priv_value))\n",
    "        unpriv_sizes.append(sum(df_party[sensitive_feature]!=priv_value))\n",
    "\n",
    "    df_with_parties=pd.concat(df_splits[0]).sort_index()\n",
    "    print(df_with_parties)\n",
    "\n",
    "    ignore_features.append('party')\n",
    "    ignore_features.append('index')\n",
    "\n",
    "    print(priv_sizes)\n",
    "    print(unpriv_sizes)\n",
    "    \n",
    "    ###########\n",
    "    # save constant parameters\n",
    "    ###########\n",
    "    \n",
    "    with open(public_path+\"/constants/parties.ini\", 'w') as f:\n",
    "        (f.write(str(parties)))\n",
    "    with open(public_path+\"/constants/repairs.ini\", 'w') as f:\n",
    "        (f.write(str(repairs)))\n",
    "    with open(public_path+\"/constants/dataset_name.ini\", 'w') as f:\n",
    "        (f.write(str(dataset_name)))\n",
    "    with open(public_path+\"/constants/lam.ini\", 'w') as f:\n",
    "        (f.write(str(lams)))\n",
    "    with open(public_path+\"/constants/features.ini\", 'w') as f:\n",
    "        (f.write(str(features)))\n",
    "    with open(public_path+\"/constants/ignore_features.ini\", 'w') as f:\n",
    "        (f.write(str(ignore_features)))\n",
    "    with open(public_path+\"/constants/sensitive_feature.ini\", 'w') as f:\n",
    "        (f.write(sensitive_feature+\",\"+str(priv_value)))\n",
    "    with open(public_path+\"/constants/target_feature.ini\", 'w') as f:\n",
    "        (f.write(target_feature))\n",
    "    with open(public_path+\"/constants/alphas.ini\", 'w') as f:\n",
    "        (f.write(str(alphas)))\n",
    "    with open(public_path+\"/constants/betas.ini\", 'w') as f:\n",
    "        (f.write(str(betas)))\n",
    "    with open(public_path+\"/constants/priv_sizes.ini\", 'w') as f:\n",
    "        (f.write(str(priv_sizes)))\n",
    "    with open(public_path+\"/constants/unpriv_sizes.ini\", 'w') as f:\n",
    "        (f.write(str(unpriv_sizes)))\n",
    "\n",
    "\n",
    "    \n",
    "    ###########\n",
    "    # loop through bin_nums_list\n",
    "    ###########\n",
    "        \n",
    "    num_train_test_splits=10\n",
    "    train_percent=2.0/3.0\n",
    "\n",
    "    with open(os.getcwd()+ model_folder_name +'/gran_results_' + dataset_name + \"_\" + sensitive_feature+ \"_p\" + str(parties) + \".csv\", 'a') as f:\n",
    "        f.write(\"dataset_name,sensitive_feature,priv_value,lam,max_bin,bin_nums,parties,n_u,n_v,priv_sizes,unpriv_sizes,alphas,betas,number_of_features,iteration,DFNR,DFPR,SUM_DIFFS,ACC,bin_repair_for_all_lams_together_time[sec],repair_type\\n\")\n",
    "\n",
    "    \n",
    "    with open(os.getcwd()+ model_folder_name +'/results_' + dataset_name + \"_\" + sensitive_feature+ \"_p\" + str(parties) + \".csv\", 'a') as f:\n",
    "        f.write(\"dataset_name,sensitive_feature,priv_value,lam,max_bin,bin_nums,parties,n_u,n_v,priv_sizes,unpriv_sizes,alphas,betas,number_of_features,AVG_DFNR,AVG_DFPR,SUM_DIFFS,AVG_ACC,bin_repair_for_all_lams_together_time[sec],repair_type\\n\")\n",
    "\n",
    "\n",
    "    ###########\n",
    "    # loop through bin_nums_list\n",
    "    #     change bin_nums.ini\n",
    "    #     run *3* (or more) parties' notebooks\n",
    "    #     loop through lambda values\n",
    "    #         create train-test splits\n",
    "    #         run ML model\n",
    "    #         record results\n",
    "    ###########\n",
    "\n",
    "\n",
    "    #########\n",
    "    # loop through bin_nums_list\n",
    "    #########\n",
    "\n",
    "    for bin_nums in bin_nums_list:\n",
    "    #for bin_nums in [bin_nums_list[0]]:\n",
    "\n",
    "        bin_repair_for_all_lams_together_start_time = time.time()\n",
    "\n",
    "        #########\n",
    "        # change bin_nums.ini\n",
    "        #########\n",
    "        with open(public_path+\"/constants/bin_nums.ini\", 'w') as f:\n",
    "            (f.write(str(bin_nums)))\n",
    "\n",
    "        #########\n",
    "        # convert notebooks to python files\n",
    "        #########\n",
    "        for i in range(0,parties):\n",
    "            player_py_path='MyMultiDistExp_player' + str(i) + '.ipynb'\n",
    "            print(player_py_path)\n",
    "            python_command = 'jupyter nbconvert --to python ' + player_py_path\n",
    "            process = subprocess.Popen(python_command.split(), stdout=subprocess.PIPE)\n",
    "\n",
    "        #########\n",
    "        # run *3* (or more) python files - one for each party\n",
    "        #########\n",
    "        python_command=\"\"\n",
    "        for i in range(0,parties):\n",
    "            player_py_path='MyMultiDistExp_player' + str(i) + '.py'\n",
    "            python_command = python_command + ' start python ' + player_py_path + ' &'\n",
    "        python_command=python_command[1:-1]\n",
    "        !$python_command\n",
    "\n",
    "        bin_repair_for_all_lams_together_time = time.time()-bin_repair_for_all_lams_together_start_time\n",
    "\n",
    "        #########\n",
    "        #loop through lambda values and repair types\n",
    "        #########\n",
    "        for repair_type in repairs:\n",
    "            for lam in lams:\n",
    "                #################\n",
    "                ##concat all datasets - to test ML's performance on repaired dataset\n",
    "                #################\n",
    "                print('####################################')\n",
    "                print('#Datasets of parties:')\n",
    "                print('####################################')\n",
    "                df_full_repaired = pd.DataFrame({}) \n",
    "                for i in range(0,parties):\n",
    "                    df_tmp_repaired=pd.read_table(private_path+\"/party\"+str(i)+\"/\" + dataset_name + \"_sens-\" + str(sensitive_feature) + \"_rep_lam\"+str(lam)+\"_bins\" + str(bin_nums[0]) + \"_rep_type\" + repair_type + \".csv\",delimiter=\",\",index_col=\"index\")\n",
    "                    print(df_tmp_repaired)\n",
    "                    df_full_repaired=pd.concat([df_full_repaired,df_tmp_repaired])\n",
    "\n",
    "                df_full_repaired=df_full_repaired.sort_index()\n",
    "\n",
    "                ################\n",
    "                ##create train-test splits from the repaired dataset\n",
    "                ##execute ML model on each split\n",
    "                ################\n",
    "\n",
    "                df_train_test_splits=create_train_test_splits(num_train_test_splits,train_percent,df_full_repaired)\n",
    "\n",
    "                Sum_DFNR=0\n",
    "                Sum_DFPR=0\n",
    "                Sum_Acc=0\n",
    "\n",
    "                for i in range(0,num_train_test_splits):\n",
    "                    train=df_train_test_splits[i][0]\n",
    "                    test=df_train_test_splits[i][1]\n",
    "                    train.insert(len(train.columns),\"isTraining\",1)\n",
    "                    test.insert(len(test.columns),\"isTraining\",0)\n",
    "\n",
    "                    df_full_repaired_t=pd.concat([train,test])\n",
    "                    print('####################################')\n",
    "                    print('#Full dataset:')\n",
    "                    print('####################################')\n",
    "                    print(df_full_repaired_t)\n",
    "\n",
    "                    #####################################\n",
    "                    ## Here change values such that the y vectors will have 1 where it is equal to positive_class_val and 0 elsewhere.\n",
    "                    ######################################\n",
    "\n",
    "                    y_train=np.array(df_full_repaired_t[target_feature][df_full_repaired_t[\"isTraining\"]==1])\n",
    "                    y_train=(y_train==positive_class_val).astype(int)\n",
    "\n",
    "                    y_test=np.array(df_full_repaired_t[target_feature][df_full_repaired_t[\"isTraining\"]==0])\n",
    "                    y_test=(y_test==positive_class_val).astype(int)\n",
    "\n",
    "\n",
    "                    sensitive_train=np.array(df_full_repaired_t[sensitive_feature][df_full_repaired_t[\"isTraining\"]==1])\n",
    "                    sensitive_test=np.array(df_full_repaired_t[sensitive_feature][df_full_repaired_t[\"isTraining\"]==0])\n",
    "\n",
    "                    more_features=[col for col in ignore_features if col not in ['party', 'index']]\n",
    "                    X_train=df_full_repaired_t[[*features,*more_features]][df_full_repaired_t[\"isTraining\"]==1]\n",
    "                    X_test=df_full_repaired_t[[*features,*more_features]][df_full_repaired_t[\"isTraining\"]==0]\n",
    "\n",
    "                    X_test_non_protected=X_test[sensitive_test==priv_value]\n",
    "                    X_test_protected=X_test[sensitive_test!=priv_value]\n",
    "\n",
    "                    y_test_non_protected=y_test[sensitive_test==priv_value]\n",
    "                    y_test_protected=y_test[sensitive_test!=priv_value]\n",
    "\n",
    "        \n",
    "                    #****************************\n",
    "                    # LR model\n",
    "                    #****************************\n",
    "\n",
    "                    logreg = LogisticRegression(penalty='l2',max_iter=1000)\n",
    "                    logreg.fit(X_train, y_train)\n",
    "                    print('Regular LR:')\n",
    "                    \n",
    "                    test_reslts=logreg.predict(X_test)\n",
    "                    print(\"dataset: \" + dataset_name + \" repair: \"+repair_type+\" bins:\" + str(bin_nums) + \" lam:\" + str(lam))\n",
    "                    print(test_reslts)\n",
    "                    test_accuracy=accuracy_score(y_test, test_reslts)\n",
    "                    print(test_accuracy)\n",
    "\n",
    "\n",
    "                    \n",
    "                    ###################\n",
    "                    # compute results\n",
    "                    ###################\n",
    "\n",
    "                    print('####################################')\n",
    "                    print('#y_test_non_protected:')\n",
    "                    print('####################################')\n",
    "                    print(y_test_non_protected)\n",
    "                    print('####################################')\n",
    "                    print('#y_pred_non_protected:')\n",
    "                    print('####################################')\n",
    "                    y_pred_non_protected=logreg.predict(X_test_non_protected)\n",
    "                    print(y_pred_non_protected)\n",
    "\n",
    "\n",
    "                    print('####################################')\n",
    "                    print('#cnf_matrix - non_protected:')\n",
    "                    print('####################################')\n",
    "                    cnf_matrix=confusion_matrix(y_test_non_protected,y_pred_non_protected,labels=[0,1])\n",
    "\n",
    "                    print(cnf_matrix)\n",
    "\n",
    "                    TN_np=cnf_matrix[0,0]\n",
    "                    FP_np=cnf_matrix[0,1]\n",
    "                    FN_np=cnf_matrix[1,0]\n",
    "                    TP_np=cnf_matrix[1,1]\n",
    "                    TNR_np=(0 if TN_np==0 else TN_np/(TN_np+FP_np))\n",
    "                    TPR_np=(0 if TP_np==0 else TP_np/(TP_np+FN_np))\n",
    "                    FPR_np=(0 if FP_np==0 else FP_np/(TN_np+FP_np))      #1-TNR_np\n",
    "                    FNR_np=(0 if FN_np==0 else FN_np/(TP_np+FN_np))      #1-TPR_np\n",
    "\n",
    "                    print(\"TN:\", TN_np,\" FP:\", FP_np,\" FN:\", FN_np, \" TP:\", TP_np)\n",
    "                    print(\"TNR:\", TNR_np,\" FPR:\", FPR_np,\" FNR:\", FNR_np, \" TPR:\", TPR_np)\n",
    "\n",
    "                    print('####################################')\n",
    "                    print('#y_test_protected:')\n",
    "                    print('####################################')\n",
    "                    print(y_test_protected)        \n",
    "\n",
    "                    print('####################################')\n",
    "                    print('#y_pred_protected:')\n",
    "                    print('####################################')\n",
    "                    y_pred_protected=logreg.predict(X_test_protected)\n",
    "                    print(y_pred_protected)\n",
    "\n",
    "\n",
    "                    print('####################################')\n",
    "                    print('#cnf_matrix - protected:')\n",
    "                    print('####################################')\n",
    "                    cnf_matrix=confusion_matrix(y_test_protected,y_pred_protected,labels=[0,1])\n",
    "\n",
    "                    print(cnf_matrix)\n",
    "\n",
    "                    TN_p=cnf_matrix[0,0]\n",
    "                    FP_p=cnf_matrix[0,1]\n",
    "                    FN_p=cnf_matrix[1,0]\n",
    "                    TP_p=cnf_matrix[1,1]\n",
    "                    TNR_p=(0 if TN_p==0 else TN_p/(TN_p+FP_p))    \n",
    "                    TPR_p=(0 if TP_p==0 else TP_p/(TP_p+FN_p)) \n",
    "                    FPR_p=(0 if FP_p==0 else FP_p/(TN_p+FP_p))      #1-TNR_p\n",
    "                    FNR_p=(0 if FN_p==0 else FN_p/(TP_p+FN_p))      #1-TPR_p\n",
    "\n",
    "                    print(\"TN:\", TN_p,\" FP:\", FP_p,\" FN:\", FN_p, \" TP:\", TP_p)\n",
    "                    print(\"TNR:\", TNR_p,\" FPR:\", FPR_p,\" FNR:\", FNR_p, \" TPR:\", TPR_p)\n",
    "\n",
    "\n",
    "                    print('####################################')\n",
    "                    print('#Results for this split:')\n",
    "                    print('####################################')\n",
    "                    print('|DFNR|: ',abs(FNR_np-FNR_p))\n",
    "                    print('|DFPR|: ',abs(FPR_np-FPR_p))\n",
    "                    print('Accuracy: ',test_accuracy)\n",
    "\n",
    "\n",
    "                    Sum_DFNR=Sum_DFNR+abs(FNR_np-FNR_p)\n",
    "                    Sum_DFPR=Sum_DFPR+abs(FPR_np-FPR_p)\n",
    "                    Sum_Acc=Sum_Acc+test_accuracy\n",
    "                    \n",
    "                    DFNR=FNR_np-FNR_p\n",
    "                    DFPR=FPR_np-FPR_p\n",
    "                    SUM_DIFFS_granular=abs(DFNR)+abs(DFPR)\n",
    "                    \n",
    "                    valList_gran=[dataset_name,sensitive_feature,priv_value,lam,max(bin_nums),str(bin_nums),\n",
    "                                     parties,n_u,n_v,str(priv_sizes),str(unpriv_sizes),str(alphas),str(betas),len(features),str(i),\n",
    "                                     DFNR,DFPR,SUM_DIFFS_granular,test_accuracy,bin_repair_for_all_lams_together_time,repair_type]\n",
    "\n",
    "\n",
    "                    with open(os.getcwd()+ model_folder_name +'/gran_results_' + dataset_name + \"_\" + sensitive_feature + \"_p\" + str(parties) +\".csv\", 'a') as f:\n",
    "                        for item in valList_gran:\n",
    "                            f.write('\"%s\",' % item)\n",
    "                        f.write('\\n')\n",
    "                                        \n",
    "                    print('\\n')\n",
    "\n",
    "        #************\n",
    "        # Here we save and record all results\n",
    "        #************\n",
    "\n",
    "                AVG_DFNR=Sum_DFNR/num_train_test_splits\n",
    "                AVG_DFPR=Sum_DFPR/num_train_test_splits\n",
    "                SUM_DIFFS=abs(Sum_DFPR/num_train_test_splits)+abs(Sum_DFNR/num_train_test_splits)\n",
    "                AVG_ACC=Sum_Acc/num_train_test_splits\n",
    "                print('lambda: ', lam)\n",
    "                print('AVG_DFNR: ',AVG_DFNR)\n",
    "                print('AVG_DFPR: ',AVG_DFPR)\n",
    "                print('SUM_DIFFS: ',SUM_DIFFS)\n",
    "                print('AVG_ACC: ',AVG_ACC)\n",
    "\n",
    "\n",
    "\n",
    "                valList=[dataset_name,sensitive_feature,priv_value,lam,max(bin_nums),str(bin_nums),\n",
    "                                 parties,n_u,n_v,str(priv_sizes),str(unpriv_sizes),str(alphas),str(betas),len(features),\n",
    "                                 AVG_DFNR,AVG_DFPR,SUM_DIFFS,AVG_ACC,bin_repair_for_all_lams_together_time,repair_type]\n",
    "\n",
    "                with open(os.getcwd()+ model_folder_name +'/results_' + dataset_name + \"_\" + sensitive_feature + \"_p\" + str(parties) +\".csv\", 'a') as f:\n",
    "                    for item in valList:\n",
    "                        f.write('\"%s\",' % item)\n",
    "                    f.write('\\n')\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
